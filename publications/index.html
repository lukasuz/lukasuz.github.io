<!DOCTYPE html> <html lang=""> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Lukas Uzolas</title> <meta name="author" content="Lukas Uzolas"/> <meta name="description" content="Personal website of Lukas Uzolas "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons|Spectral:100,300,400,500,700"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Ysabeau+SC:300,400,500,700"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Ysabeau+Office:300,400,500,700"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Bricolage+Grotesque:200,300,400,500,700"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>L</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://lukas.uzolas.com/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lukas</span> Uzolas </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/"> Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <a class="nav-link" target="_blank" href="/assets/pdf/CV_Lukas_Uzolas.pdf"> CV </a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <p><i>* denotes equal contribution</i></p> <div class="publications"> <h2>Peer-reviewed</h2> <h2 class="year">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SIGGRAPH Asia</abbr></div> <div id="uzolas2025surfaceawaredistilled3dsemantic" class="col-sm-8"> <div class="title">Surface-Aware Distilled 3D Semantic Features</div> <div class="author"> <em>Uzolas, Lukas</em>, Eisemann, Elmar, and Kellnhofer, Petr </div> <div class="periodical"> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://lukas.uzolas.com/SurfaceAware3DFeatures/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://arxiv.org/abs/2503.18254" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lukasuz/SurfaceAware3DFeaturesCode" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Many 3D tasks such as pose alignment, animation, motion transfer, and 3D reconstruction rely on establishing correspondences between 3D shapes. This challenge has recently been approached by matching of semantic features from pre-trained vision models. However, despite their power, these features struggle to differentiate instances of the same semantic class such as "left hand" versus "right hand" which leads to substantial mapping errors. To solve this, we learn a surface-aware embedding space that is robust to these ambiguities. Importantly, our approach is self-supervised and requires only a small number of unpaired training meshes to infer features for new 3D shapes at test time. We achieve this by introducing a contrastive loss that preserves the semantic content of the features distilled from foundational models while disambiguating features located far apart on the shape’s surface. We observe superior performance in correspondence matching benchmarks and enable downstream applications including in-part segmentation, pose alignment, and motion transfer.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">3DV</abbr></div> <div id="uzolas2024motiondreamer" class="col-sm-8"> <div class="title">MotionDreamer: Exploring Semantic Video Diffusion features for Zero-Shot 3D Mesh Animation</div> <div class="author"> <em>Uzolas, Lukas</em>, Eisemann, Elmar, and Kellnhofer, Petr </div> <div class="periodical"> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://lukas.uzolas.com/MotionDreamer/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://arxiv.org/abs/2405.20155" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lukasuz/MotionDreamer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Animation techniques bring digital 3D worlds and characters to life. However, manual animation is tedious and automated techniques are often specialized to narrow shape classes. In our work, we propose a technique for automatic re-animation of arbitrary 3D shapes based on a motion prior extracted from a video diffusion model. Unlike existing 4D generation methods, we focus solely on the motion, and we leverage an explicit mesh-based representation compatible with existing computer-graphics pipelines. Furthermore, our utilization of diffusion features enhances accuracy of our motion fitting. We analyze efficacy of these features for animation fitting and we experimentally validate our approach for two different diffusion models and four animation models. Finally, we demonstrate that our time-efficient zero-shot method achieves a superior performance re-animating a diverse set of 3D shapes when compared to existing techniques in a user study.</p> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="uzolas2023templatefree" class="col-sm-8"> <div class="title">Template-free Articulated Neural Point Clouds for Reposable View Synthesis</div> <div class="author"> <em>Uzolas, Lukas</em>, Eisemann, Elmar, and Kellnhofer, Petr </div> <div class="periodical"> <em>In Thirty-seventh Conference on Neural Information Processing Systems</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://lukas.uzolas.com/Articulated-Point-NeRF/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://arxiv.org/abs/2305.19065" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lukasuz/Articulated-Point-NeRF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when synthesizing novel views of time-evolving 3D scenes. However, the common reliance on backward deformation fields makes reanimation of the captured object poses challenging. Moreover, the state of the art dynamic models are often limited by low visual fidelity, long reconstruction time or specificity to narrow application domains. In this paper, we present a novel method utilizing a point-based representation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an associated skeletal model from even sparse multi-view video. Our forward-warping approach achieves state-of-the-art visual fidelity when synthesizing novel views and poses while significantly reducing the necessary learning time when compared to existing work. We demonstrate the versatility of our representation on a variety of articulated objects from common datasets and obtain reposable 3D reconstructions without the need of object-specific skeletal templates.</p> </div> </div> </div> </li></ol> <h2 class="year">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE Access</abbr></div> <div id="9785372" class="col-sm-8"> <div class="title">Deep Anomaly Generation: An Image Translation Approach of Synthesizing Abnormal Banded Chromosome Images</div> <div class="author">Uzolas, Lukas*, Rico, Javier*, Coupé, Pierrick, Sanmiguel, Juan C., and Cserey, György </div> <div class="periodical"> <em>IEEE Access</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9785372" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/lukasuz/Banding-Pattern-Extraction" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p> Advances in deep-learning-based pipelines have led to breakthroughs in a variety of microscopy image diagnostics. However, a sufficiently big training data set is usually difficult to obtain due to high annotation costs. In the case of banded chromosome images, the creation of big enough libraries is difficult for multiple pathologies due to the rarity of certain genetic disorders. Generative Adversarial Networks (GANs) have proven to be effective in generating synthetic images and extending training data sets. In our work, we implement a conditional GAN (cGAN) that allows generation of realistic single chromosome images following user-defined banding patterns. To this end, an image-to-image translation approach based on automatically created 2D chromosome segmentation label maps is used. Our validation shows promising results when synthesizing chromosomes with seen as well as unseen banding patterns. We believe that this approach can be exploited for data augmentation of chromosome data sets with structural abnormalities. Therefore, the proposed method could help to tackle medical image analysis problems such as data simulation, segmentation, detection, or classification in the field of cytogenetics. </p> </div> </div> </div> </li></ol> <h2 class="year">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MuC</abbr></div> <div id="boysen2018scale" class="col-sm-8"> <div class="title">Scale &amp; Walk: Evaluation of scaling-based interaction techniques for natural locomotion in VR (German Original: Scale &amp; Walk: Evaluation von skalierungsbasierten Interaktionstechniken zur natürlichen Fortbewegung in VR)</div> <div class="author">Boysen, Yannic*, Husung, Malte*, Mantei, Timo*, Müller, Lisa-Maria*, Schimmelpfennig, Joshua*, Uzolas, Lukas*, and Langbehn, Eike </div> <div class="periodical"> <em>Mensch und Computer 2018-Tagungsband</em> 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.gi.de/handle/20.500.12116/16731" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p> Virtual reality headsets, such as the HTC Vive, enable the user to move around in the virtual world through real movements. However, this is only applicable to a limited extent, as the walkable real space is usually significantly smaller than the virtual space. Scaling techniques make it possible to travel long distances in the virtual world by manipulating the virtual size of the user. In this paper, we present an experiment in which we compare two scaling techniques versus accelerated walking on the basis of usability, sense of presence, motion sickness and spatial understanding. Our results show that automatic scaling in its current form performs significantly worse in terms of usability and motion sickness than accelerated walking and self-determined scaling. Self-determined scaling, however, is an equivalent alternative to accelerated walking. </p> </div> </div> </div> </li></ol> <div class="publications"> <h2>Theses</h2> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">M.Sc.</abbr></div> <div id="UzolasThesis2021" class="col-sm-8"> <div class="title">Meta-Learning for Domain Generalization with Style-based Parameter Prediction for Biomedical Image Segmentation</div> <div class="author"> <em>Uzolas, Lukas</em> </div> <div class="periodical"> <em>University of Bordeaux </em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://drive.google.com/file/d/1CI4kKt91Z3HXowuXS0nEYo-1poie35qr/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Deep Learning models often suffer a degradation in performance when applied to data sets sampled from a different distribution than the training data set. For example, this shift in distributions can be induced by different imaging devices located at different hospitals, producing images of different resolution, contrast, and brightness. Several well-established solutions exist to tackle this problem by aligning the distributions between data sets but these approaches necessitate that the target distribution is known a priori which is not always realistic. Meta-Learning for Domain Generalization can train models that generalize well on unseen target data by inducing a domain shift during training. However, the importance of normalization layers in these models has been neglected, yet regulation of normalization parameters has been proven to be beneficial to tackle the domain shift problem. To this end, this thesis aims to investigate whether a normalization parameter prediction scheme can improve generalization performance on an unseen target data set while using a Meta-Learning approach. In this context, the source data is defined as the data available during training, while the target data is unknown; the Domain shift described the shift in distribution between these two sets. We investigate two parameter prediction methods: Firstly, recently introduced Instance-Level Meta Normalization which predicts the scale and shift on a local level based on the feature map moments (mean and variance). Secondly, we propose a global parameter prediction scheme based on embeddings from a pre-trained Inception-v3, inspired by contemporary work in Neural Style Transfer. To evaluate the methods, we generate a shape data set characterized by the same underlying content, which only differs in style between domains. Our results are twofold: We find that the global prediction scheme outperforms Instance-Level Meta Normalization on our shape data set, improving generalization marginally and convergence significantly. Additionally, we discover that the Meta-Learning approach results in worse performance when compared to a traditional supervised training. We, thus, show that the domain shift problem can be partially handled by predicting the normalization parameters based on the input, consistent with the findings of related works.</p> </div> </div> </div> </li></ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Lukas Uzolas. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>