<!DOCTYPE html> <html lang=""> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Face animations without training | Lukas Uzolas</title> <meta name="author" content="Lukas Uzolas"/> <meta name="description" content="Extending facial landmark projection to produce animations"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons|Spectral:100,300,400,500,700"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Ysabeau+SC:300,400,500,700"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Ysabeau+Office:300,400,500,700"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Bricolage+Grotesque:200,300,400,500,700"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>L</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://lukas.uzolas.com/blog/2021/animation/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lukas</span> Uzolas </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/"> Publications </a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Face animations without training</h1> <p class="post-meta">October 17, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a> </p> </header> <article class="post-content"> <p><em>If you do not have any previous knowledge in projecting images into the latent space of StyleGAN, you should read [3] and [5] first.</em></p> <p>In my <a href="/blog/2021/landmarks/">previous post</a>, I showed how to project facial landmarks into the latent space of a pre-trained StyleGAN2 while keeping the look of another one (to some extent), i.e. the identity. In this post, I will explain how to expand this approach for generating short face animations in two ways. The first approach keeps a face identity, while the second one does <strong>not</strong> keep a face identity. Allowing for smooth transitions between faces while projecting coherent facial animation into the latent space. Note that no model was trained for this approach and it is purely done through minimization, thus the quality of generation is limited. I will go into the limitations of this methods later in this post.</p> <p>You can generate your own animations in this <a href="https://colab.research.google.com/drive/1ghSB78uobrRxWRtDSCsXCKEUOkUlpOyH?usp=sharing" target="_blank" rel="noopener noreferrer">notebook</a>. The code itself can be found in this <a href="https://github.com/lukasuz/stylegan2-landmark-projection" target="_blank" rel="noopener noreferrer">repository</a>.</p> <p><strong>I have significantly improved the results. A sample sequence is given at the end of the post.</strong></p> <h3>Preserving the face identity (more or less)</h3> <p>For this, we will need two inputs. Firstly, a face image, \(x_{look}\), whose look shall be replicated. Secondly, a sequence of \(i\) images, \(x_{landmark}^i\), whose facial landmarks are used for projection. Examples are given below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/look_img.png"> </div> <div class="col-sm mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" width="100%" controls="" autoplay="" loop=""> <source src="/assets/img/scream_input.m4v" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> <div class="col-sm mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" width="100%" controls="" autoplay="" loop=""> <source src="/assets/img/scream_output.m4v" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> </div> <div class="caption"> Example 1: Left the look image (<a href="https://github.com/NVlabs/ffhq-dataset" target="_blank" rel="noopener noreferrer">FFHQ</a>), middle the landmark video (<a href="https://www.pexels.com/video/a-woman-screaming-8724361/" target="_blank" rel="noopener noreferrer">pexels</a>), right the output. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/look_img.png"> </div> <div class="col-sm mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" width="100%" controls="" autoplay="" loop=""> <source src="/assets/img/smile_input.m4v" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> <div class="col-sm mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" width="100%" controls="" autoplay="" loop=""> <source src="/assets/img/smile_output.m4v" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> </div> <div class="caption"> Example 2: Left the look image (<a href="https://github.com/NVlabs/ffhq-dataset" target="_blank" rel="noopener noreferrer">FFHQ</a>), middle the landmark video (<a href="https://www.pexels.com/video/" target="_blank" rel="noopener noreferrer">pexels</a>), right the output. </div> <p>For each video frame index \(i\) we want to find the latent code \(w^i\), with \(f(w^i) = x^i\), where \(x^i\) is a face image and \(f\) the StyleGAN face generation network, which minimizes a loss function made up of three terms:</p> \[\arg \min_{w^i} \lambda_{1} \mathcal{L}_{lpips}(x_{look}, x^i) + \lambda_{2} \mathcal{L}_{fan}(x_{landmark}^i, x^i, \lambda_{landmark}) + \lambda_{3} \\ \mathcal{L}_{smooth}(w^i, w^{i-1}).\] <p>Practically, this is achieved through gradient descent, keeping the network weights fixed, but treating the input \(w^i\) variable. The first term measures the perceptual similarity between generated images and the target look, see [2]. The second term captures the divergence between facial landmarks of the current generated image and the current target video frame. The last term calculates the similarity between the current latent code and the previous latent code. More specifically, the second term is defined as:</p> \[\mathcal{L}_{fan}(x^1, x^2, \lambda_{landmark}) = \sum_i^N \lambda_{landmark} \sqrt{(FAN(x^1) - FAN(x^2))^2},\] <p>where <em>N</em> is the number of pixels, and <strong><em>FAN</em></strong> is the landmark heat map extraction model [1] which outputs a three-dimensional matrix, \(\mathcal{R}^{64x64xc}\), where the last dimension corresponds to the landmarks and the first two correspond to the spatial dimensions. Note that \(\lambda_{landmark}\) is a <strong>vector</strong> containing the weights for each group of landmarks. Groups are, for example, Eyebrows, eyes, mouth, etc. Check [1] for more info. By tweaking this vector you can determine what facial features you want to project <strong>more strongly</strong> into the generated images. For example, drop any influence of the eyes, but keep mouth movement.</p> <p>The smoothness term is defined as the l2 norm between consecutive latent codes:</p> \[\mathcal{L}_{smooth}(w^1, w^2) = ||w^1-w^2||_2.\] <p>It helps to keep features stable that are not directly constrained by the perceptual loss or the heat map loss more stable over time. It encourages \(w^i\) to stay close to its predecessor.</p> <p>During the first iteration, \(w^i\) is initialized with \(w^i=\hat{w}\), which is the mean latent vector from randomly sampled latent codes [3]. It corresponds to the average face based on the data set. Furthermore, we iterate for the first image \(iter_{first}=300\) times. Even though \(1000\) iterations are suggested in [3], it yields decent results here and is preferably due to my limited resources.</p> <p>For consecutive frame indices, \(i &gt; 0\), we initialize the current latent code with the previous latent code, \(w_i = w_{i-1}\). As the next frame should be relatively similar to the previous one, we can reduce the number of iterations for consecutive frames, \(iter_{consecutive}=50\). Note that I choose these values empirically, based on a trade-off between quality and generation time. With more iterations (i.e more computing power), the results probably would get better.</p> <p>For further smoothing the video, subframes are generated which effectively double the number of frames per second. This is possible due to the interpolation capabilities of the latent space. A subframe is generated by linear interpolating neighbouring latent codes \(w^{i,i+1} = \frac{1}{2}w^i + \frac{1}{2}w^{i+1}\), the corresponding frame is generated by \(f(w^{i,i+1}) = x^{i,i+1}\). Theoretically, we could increase the sampling rate between latent codes for more smoothness, but a single interpolation point seems to do the job quite well.</p> <h4>Limitations</h4> <p>Several problems exist with this approach:</p> <ul> <li>First of all, as you can see the cropped face video is noisy and therefore quite wiggly, which results in this wiggling being present in the projected video as well. Temporal smoothing will most likely help here.</li> <li>Second, the identity is not kept in the course of the sequence. I believe that the extracted landmarks actually carry some signal from the image, thus we project conflicting information into the latent space. An “optimal” representation of one image’s look while expressing the facial landmarks from another one does not seem to be completely possible with this approach. This was already visible from the previous post. Perhaps, the introduction of a Person Reid loss would help. Furthermore, shape and pose are entangled in the facial landmarks which results in changing the shape of eyes and mouth in the projected image, when comparing with the input look image.</li> <li>Thirdly, facial landmarks are ambiguous. For example, a laughing face’s landmarks might be identical to a face with bigger lips.</li> <li>Next, the weighting factors of each term have a big influence on the results and usually have to be adapted for each video somewhat. While a stronger smoothness factor might keep less constrained features (e.g. lighting and hair) constant over the sequence, it actually can result in the landmarks not being projected at all.</li> <li>Lastly, it is relatively slow, especially because I am running it on Colab. A sequence of a second takes around 20 minutes to generate.</li> </ul> <p>This approach can not compete quality-wise with s.o.t.a. StyleGAN-based face animation approaches. Nevertheless, I am happy with the quality of the results. As no training of networks is necessary, it works on Colab, only a single look image is necessary, and no dataset-based analysis of the latent space is necessary.</p> <p>Due to not being able to preserve identity, I prefer the second option explained in the following section.</p> <h3>Disregarding the face identity</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> </div> <div class="col-sm mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" width="100%" controls="" autoplay="" loop=""> <source src="/assets/img/animation_wo_identity.m4v" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> <div class="col-sm mt-3 mt-md-0"> </div> </div> <div class="caption"> Video projection without a look image. I used a sequence of myself here. </div> <p>In this case, we do a simple modification to the minimization term, where we drop the perceptual similarity term, but add a face fidelity term:</p> \[\arg \min_{w^i} \lambda_{2} \mathcal{L}_{fan}(x_{landmark}^i, x^i, \lambda_{landmark}) + \lambda_{3} \mathcal{L}_{smooth}(w^i, w^{i-1}) + \lambda_{4} \mathcal{L}_{fidelity}(w^i, \hat{w}).\] <p>The face fidelity term is defined as the l2 norm between the current latent code and the mean latent code, similar to the smoothness term. This term is necessary, as the quality deteriorates very quickly when we do not constrain the latent space. This is based on the truncation idea proposed in [4], where truncation of \(w\) results in images with better quality while losing some variation in faces.</p> <p>Looking at the video, we can see that the frames are a little bit smoother when compared to the previous case. The face also smoothly changes the identity in the course of the sequence. However, the faces exhibit relatively similar features, i.e. male, beard, curly hair. This is interesting to see, as I also have a beard and I used a sequence of myself in this use case. This further strengthens the assumption that the output of the FAN network carries some signal from the input image.</p> <p>I am looking forward to trying this out in the future with the improved StyleGAN3 model.</p> <p>Again, a huge shout out to the StyleGAN-team and NVIDIA for their work and pre-trained models.</p> <h3>Improved Results</h3> <p>I have been able to improve the result with respect to two things. Firstly, the sequence is significantly less jittery. This can be achieved by fixing the noise buffers after the first iteration. The remaining jittering can be explained by the unsmoothed sequences of extracted facial landmarks. One remaining problem is the variable light source. Secondly, the projected image is closer in look to the target by seachring space for the latent code to \(18 \times 512\), i.e. a different latent code \(1 \times 512\) for each layer in generator. This expansion of the latent space results often in weird artifacts in the generated images, but the fidelity loss term can be utilized to circumvent this. Besides this, I upgraded to ColabPro and a 1 second video can now be generated within 6-7 minutes. I highly recommend it. Anyway, here the results:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/look_img.png"> </div> <div class="col-sm mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" width="100%" controls="" autoplay="" loop=""> <source src="/assets/img/smile_input.m4v" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> <div class="col-sm mt-3 mt-md-0"> <video class="img-fluid rounded z-depth-1" width="100%" controls="" autoplay="" loop=""> <source src="/assets/img/improved_smile.m4v" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> </div> <div class="caption"> Improved Example 3: Left the look image (<a href="https://github.com/NVlabs/ffhq-dataset" target="_blank" rel="noopener noreferrer">FFHQ</a>), middle the landmark video (<a href="https://www.pexels.com/video/" target="_blank" rel="noopener noreferrer">pexels</a>), right the output. </div> <h3 id="references">References</h3> <p>[0]: Karras, Tero, et al. “Training generative adversarial networks with limited data.” <em>arXiv preprint arXiv:2006.06676</em> (2020). Code: https://github.com/NVlabs/stylegan2-ada-pytorch</p> <p>[1]: Bulat, Adrian, and Georgios Tzimiropoulos. “How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks).” <em>Proceedings of the IEEE International Conference on Computer Vision</em>. 2017. Code: https://github.com/1adrianb/face-alignment</p> <p>[2]: Zhang, Richard, et al. “The unreasonable effectiveness of deep features as a perceptual metric.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2018.</p> <p>[3]: Karras, Tero, et al. “Analyzing and improving the image quality of stylegan.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p> <p>[4]: Karras, Tero, Samuli Laine, and Timo Aila. “A style-based generator architecture for generative adversarial networks.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</p> <p>[5]: Abdal, Rameen, Yipeng Qin, and Peter Wonka. “Image2stylegan: How to embed images into the stylegan latent space?.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Lukas Uzolas. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>