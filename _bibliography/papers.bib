---
---

@inproceedings{
  uzolas2023templatefree,
  title={Template-free Articulated Neural Point Clouds for Reposable View Synthesis},
  author={Lukas Uzolas and Elmar Eisemann and Petr Kellnhofer},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=fyfmHi8ay3},
  code={https://github.com/lukasuz/Articulated-Point-NeRF},
  pdf={https://arxiv.org/abs/2305.19065},
  abbr={NeurIPS},
  html={https://lukas.uzolas.com/Articulated-Point-NeRF/},
  abstract={Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when synthesizing novel views of time-evolving 3D scenes. However, the common reliance on backward deformation fields makes reanimation of the captured object poses challenging. Moreover, the state of the art dynamic models are often limited by low visual fidelity, long reconstruction time or specificity to narrow application domains. In this paper, we present a novel method utilizing a point-based representation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an associated skeletal model from even sparse multi-view video. Our forward-warping approach achieves state-of-the-art visual fidelity when synthesizing novel views and poses while significantly reducing the necessary learning time when compared to existing work. We demonstrate the versatility of our representation on a variety of articulated objects from common datasets and obtain reposable 3D reconstructions without the need of object-specific skeletal templates.}
}

@ARTICLE{9785372,
  author={Uzolas, Lukas* and Rico, Javier* and Coup&#x00E9;, Pierrick and Sanmiguel, Juan C. and Cserey, Gy&#x00F6;rgy},
  journal={IEEE Access}, 
  title={Deep Anomaly Generation: An Image Translation Approach of Synthesizing Abnormal Banded Chromosome Images}, 
  year={2022},
  volume={},
  number={},
  bibtex_type={journal},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9785372},
  code={https://github.com/lukasuz/Banding-Pattern-Extraction},
  abbr={IEEE Access},
  pages={1-1},
  abstract={
Advances in deep-learning-based pipelines have led to breakthroughs in a variety of microscopy image diagnostics. However, a sufficiently big training data set is usually difficult to obtain due to high annotation costs. In the case of banded chromosome images, the creation of big enough libraries is difficult for multiple pathologies due to the rarity of certain genetic disorders. Generative Adversarial Networks (GANs) have proven to be effective in generating synthetic images and extending training data sets. In our work, we implement a conditional GAN (cGAN) that allows generation of realistic single chromosome images following user-defined banding patterns. To this end, an image-to-image translation approach based on automatically created 2D chromosome segmentation label maps is used. Our validation shows promising results when synthesizing chromosomes with seen as well as unseen banding patterns. We believe that this approach can be exploited for data augmentation of chromosome data sets with structural abnormalities. Therefore, the proposed method could help to tackle medical image analysis problems such as data simulation, segmentation, detection, or classification in the field of cytogenetics.  },
  doi={10.1109/ACCESS.2022.3178786}}


@article{boysen2018scale,
  title={Scale \& Walk: Evaluation of scaling-based interaction techniques for natural locomotion in VR (German Original: Scale \& Walk: Evaluation von skalierungsbasierten Interaktionstechniken zur nat{\"u}rlichen Fortbewegung in VR)},
  author={Boysen, Yannic* and Husung, Malte* and Mantei, Timo* and M{\"u}ller, Lisa-Maria* and Schimmelpfennig, Joshua* and Uzolas, Lukas* and Langbehn, Eike},
  journal={Mensch und Computer 2018-Tagungsband},
  year={2018},
  publisher={Gesellschaft f{\"u}r Informatik eV},
  bibtex_type={conference},
  abbr={MuC},
  pdf={https://dl.gi.de/handle/20.500.12116/16731},
  abstract={
    Virtual reality headsets, such as the HTC Vive, enable the user to move around in the virtual world through real movements. However, this is only applicable to a limited extent, as the walkable real space is usually significantly smaller than the virtual space. Scaling techniques make it possible to travel long distances in the virtual world by manipulating the virtual size of the user. In this paper, we present an experiment in which we compare two scaling techniques versus accelerated walking on the basis of usability, sense of presence, motion sickness and spatial understanding. Our results show that automatic scaling in its current form performs significantly worse in terms of usability and motion sickness than accelerated walking and self-determined scaling. Self-determined scaling, however, is an equivalent alternative to accelerated walking.
  }
}
